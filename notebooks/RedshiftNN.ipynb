{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD, Adagrad, Adamax, RMSprop\n",
    "#from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l1, l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(bands=['u','g','i','r','z'], use_stokes=False, use_averages=False, use_normal_colour_index=False):\n",
    "    features = []\n",
    "\n",
    "    base_features = [\n",
    "                      'dered',\n",
    "                      'petroRad',\n",
    "                      'petroR50',\n",
    "                      'petroR90',\n",
    "                      'petro_R90_R50_ratio',\n",
    "                      'petroMag',\n",
    "                    ]\n",
    "    \n",
    "    stokes_features = [\n",
    "                      'stokes_q',\n",
    "                      'stokes_u',\n",
    "                      'stokes_p'\n",
    "                      ]\n",
    "\n",
    "    average_features = [\n",
    "        'avg_petro_rad',\n",
    "        'avg_petro_R50',\n",
    "        'avg_petro_R90',\n",
    "        'avg_petro_R90_R50_ratio'\n",
    "    ]\n",
    "    \n",
    "    average_stokes_features = [\n",
    "        'avg_stokes_q',\n",
    "        'avg_stokes_u',\n",
    "    ]\n",
    "    \n",
    "    valid_colour_indexes = [\n",
    "        'u_g_colour_index',\n",
    "        'g_r_colour_index',\n",
    "        'r_i_colour_index',\n",
    "        'i_z_colour_index',\n",
    "    ]\n",
    "    \n",
    "    for band in bands:\n",
    "        for base_feature in base_features:\n",
    "            feature = '{}_{}'.format(base_feature, band)\n",
    "            features.append(feature)\n",
    "            \n",
    "        if use_stokes:\n",
    "            for stokes_feature in stokes_features:\n",
    "                feature = '{}_{}'.format(stokes_feature, band)\n",
    "                features.append(feature)\n",
    "        \n",
    "        for band2 in bands:\n",
    "            feature = '{}_{}_colour_index'.format(band, band2)\n",
    "            if feature in valid_colour_indexes:\n",
    "                petro_feature = 'petro_{}'.format(feature)\n",
    "                features.append(petro_feature)\n",
    "                if use_normal_colour_index:\n",
    "                    features.append(feature)\n",
    "\n",
    "    if use_averages:\n",
    "        features.extend(average_features)\n",
    "        if use_stokes:\n",
    "            features.extend(average_stokes_features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIRIAL_GALAXY_TYPE    = 0\n",
    "ELLIPTICAL_GALAXY_TYPE = 1\n",
    "UNKNOWN_GALAXY_TYPE    = 2\n",
    "\n",
    "features = generate_features(use_normal_colour_index=False)\n",
    "\n",
    "target_column = 'z'\n",
    "\n",
    "CONFIDENCE_LEVEL = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('data/input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input_data.copy()\n",
    "data = data[np.all([data.z <= 0.4, data.z >= 0], axis=0)]\n",
    "combined_spiral = data.spiralclock + data.spiralanticlock + data.edgeon\n",
    "data['galaxy_type'] = UNKNOWN_GALAXY_TYPE\n",
    "data['combined_spiral'] = combined_spiral\n",
    "data.loc[data.debiased_elliptical > CONFIDENCE_LEVEL, 'galaxy_type'] = ELLIPTICAL_GALAXY_TYPE\n",
    "data.loc[data.debiased_spiral > CONFIDENCE_LEVEL, 'galaxy_type'] = SPIRIAL_GALAXY_TYPE\n",
    "\n",
    "# Add petroR50/petroR90\n",
    "data['petro_R90_R50_ratio_u'] = data.petroR90_u / data.petroR50_u\n",
    "data['petro_R90_R50_ratio_g'] = data.petroR90_g / data.petroR50_g\n",
    "data['petro_R90_R50_ratio_r'] = data.petroR90_r / data.petroR50_r\n",
    "data['petro_R90_R50_ratio_i'] = data.petroR90_i / data.petroR50_i\n",
    "data['petro_R90_R50_ratio_z'] = data.petroR90_z / data.petroR50_z\n",
    "data['avg_petro_rad'] = (data.petroRad_u + data.petroRad_g + data.petroRad_r + data.petroRad_i + data.petroRad_z)/5\n",
    "data['avg_petro_R50'] = (data.petroR50_u + data.petroR50_g + data.petroR50_r + data.petroR50_i + data.petroR50_z)/5\n",
    "data['avg_petro_R90'] = (data.petroR90_u + data.petroR90_g + data.petroR90_r + data.petroR90_i + data.petroR90_z)/5\n",
    "data['avg_petro_R90_R50_ratio'] = data.avg_petro_R90 / data.avg_petro_R50\n",
    "\n",
    "data['u_g_colour_index'] = data.dered_u - data.dered_g\n",
    "data['g_r_colour_index'] = data.dered_g - data.dered_r\n",
    "data['r_i_colour_index'] = data.dered_r - data.dered_i\n",
    "data['i_z_colour_index'] = data.dered_i - data.dered_z\n",
    "\n",
    "# does average of stokes in different bands really matter?\n",
    "data['avg_stokes_u'] = (data.stokes_u_u + data.stokes_u_g + data.stokes_u_r + data.stokes_u_i + data.stokes_u_z)/5\n",
    "data['avg_stokes_q'] = (data.stokes_q_u + data.stokes_q_g + data.stokes_q_r + data.stokes_q_i + data.stokes_q_z)/5\n",
    "\n",
    "# Average of petro rad\n",
    "data['avg_petro_rad'] = (data.petroRad_u + data.petroRad_g + data.petroRad_r + data.petroRad_i + data.petroRad_z)/5\n",
    "\n",
    "# Petro Mag colour index\n",
    "data['petro_u_g_colour_index'] = data.petroMag_u - data.petroMag_g\n",
    "data['petro_g_r_colour_index'] = data.petroMag_g - data.petroMag_r\n",
    "data['petro_r_i_colour_index'] = data.petroMag_r - data.petroMag_i\n",
    "data['petro_i_z_colour_index'] = data.petroMag_i - data.petroMag_z\n",
    "\n",
    "# Stokes P\n",
    "data['stokes_p_u'] = np.sqrt(np.power(data.stokes_q_u, 2) + np.power(data.stokes_u_u, 2))\n",
    "data['stokes_p_g'] = np.sqrt(np.power(data.stokes_q_g, 2) + np.power(data.stokes_u_g, 2))\n",
    "data['stokes_p_i'] = np.sqrt(np.power(data.stokes_q_i, 2) + np.power(data.stokes_u_i, 2))\n",
    "data['stokes_p_r'] = np.sqrt(np.power(data.stokes_q_r, 2) + np.power(data.stokes_u_r, 2))\n",
    "data['stokes_p_z'] = np.sqrt(np.power(data.stokes_q_z, 2) + np.power(data.stokes_u_z, 2))\n",
    "\n",
    "num_of_elliptical = data[data.galaxy_type == ELLIPTICAL_GALAXY_TYPE].size\n",
    "num_of_spirial = data[data.galaxy_type == SPIRIAL_GALAXY_TYPE].size\n",
    "num_of_unknown = data[data.galaxy_type == UNKNOWN_GALAXY_TYPE].size\n",
    "total_count = data.size\n",
    "\n",
    "print(num_of_elliptical / total_count)\n",
    "print(num_of_spirial / total_count)\n",
    "print(num_of_unknown / total_count)\n",
    "print(num_of_spirial / (num_of_elliptical + num_of_spirial))\n",
    "\n",
    "known_data = data[data.galaxy_type != UNKNOWN_GALAXY_TYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train(X, y, random_state=None, num_bins=24, normalise=True, min_y=-3, max_y=3, test_size=0.2):\n",
    "    # normalise y first to make sure we can bin properly\n",
    "    y_tmp = y.copy()\n",
    "    if normalise:\n",
    "        y_normaliser = PowerTransformer()\n",
    "        y_tmp = y_normaliser.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "    bins = np.linspace(min_y, max_y, num_bins)\n",
    "    y_binned = np.digitize(y_tmp, bins)\n",
    "    \n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y_binned, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_bins = 4\n",
    "# bins = np.linspace(z_min, z_max, num_bins)\n",
    "# y_binned = np.digitize(known_data, bins)\n",
    "# for bin_num in range(num_bins):\n",
    "#     print(f'bin {bin_num} has {len(y_binned[y_binned == bin_num])} records')\n",
    "\n",
    "# split = StratifiedShuffleSplit(n_splits=num_bins, test_size=0.2)\n",
    "\n",
    "# for train_index, test_index in split.split(X, y_binned):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "X = known_data[features]\n",
    "y = known_data['z'].values\n",
    "X_train, X_test, y_train, y_test = split_train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = PowerTransformer()\n",
    "y_scaler = PowerTransformer()\n",
    "X_train_norm = x_scaler.fit_transform(X_train)\n",
    "y_train_norm = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "# X_t, X_v, y_t, y_v = train_test_split(X_train_norm, y_train_norm, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_norm, bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler.lambdas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(input_shape, output_shape, dense_units=1024, dropout_rate_1=0.1, dropout_rate_2=0.3, l1_reg=0.1, l2_reg=0.01, lr=0.00001):\n",
    "    input = Input(shape=input_shape, name='input_1')\n",
    "    x = Dense(dense_units,\n",
    "              kernel_initializer='random_normal',\n",
    "#               kernel_regularizer=l2(l2_reg),\n",
    "              name='hidden_layer_1',\n",
    "              use_bias=False,\n",
    "             activation='relu'\n",
    "             )(input)\n",
    "#     x = Dropout(dropout_rate_1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_units,\n",
    "              kernel_initializer='random_normal',\n",
    "#               kernel_regularizer=l2(l2_reg),\n",
    "              name='hidden_layer_2',\n",
    "              use_bias=False,\n",
    "              activation='relu'\n",
    "             )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_units,\n",
    "              kernel_initializer='random_normal',\n",
    "#               kernel_regularizer=l2(l2_reg),\n",
    "              name='hidden_layer_3',\n",
    "              use_bias=False,\n",
    "              activation='relu'\n",
    "             )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_units,\n",
    "              kernel_initializer='random_normal',\n",
    "#               kernel_regularizer=l2(l2_reg),\n",
    "              name='hidden_layer_4',\n",
    "              use_bias=False,\n",
    "              activation='relu'\n",
    "             )(x)\n",
    "#    x = Dropout(dropout_rate_2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_shape,\n",
    "              kernel_initializer='random_normal',\n",
    "#               kernel_regularizer=l2(l2_reg),\n",
    "              use_bias=False,\n",
    "              name='output'\n",
    "             )(x)\n",
    "\n",
    "#    optimizer = SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
    "    optimizer = Adam(lr=lr)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model = create_nn((X.shape[1],), 1, dense_units=2048, lr=0.00003)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_train_validate(X, y, test_size=0.2, bins=24, random_state=1138):\n",
    "#     X_train, X_valid, y_train, y_valid = split_train(X, y, normalise=False, bins=bins, test_size=test_size, random_state=random_state)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = split_train(X_train_norm, y_train_norm, normalise=False, random_state=1138)\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "model.reset_states()\n",
    "#model.fit(X_train_norm, y_train_norm, epochs=60, batch_size=100, callbacks=callbacks, validation_split=0.2)\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_train, y_train), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test_norm), y_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "ITERATION = 0\n",
    "\n",
    "hp_out_file = 'gbm_hp_trials.csv'\n",
    "of_connection = open(hp_out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "of_connection.close()\n",
    "\n",
    "def objective(x_train, y_train, random_state=42, stratified=True):    \n",
    "    def _objective(params, n_folds=N_FOLDS):\n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        \n",
    "        print(params)\n",
    "\n",
    "        ITERATION += 1\n",
    "\n",
    "        # 1. Create + compile model\n",
    "        params['dense_units'] = int(params['dense_units'])\n",
    "        params['batch_size'] = int(params['batch_size'])\n",
    "        params['input_shape'] = (x_train.shape[1],)\n",
    "        params['output_shape'] = 1\n",
    "        callbacks = [EarlyStopping(monitor='mean_squared_error', patience=2)]\n",
    "        \n",
    "        model = KerasRegressor(build_fn=create_nn, verbose=0, epochs=100, **params)\n",
    "        \n",
    "        # 2. Do Cross Validation\n",
    "        \n",
    "        start = timer()\n",
    "        cv = ShuffleSplit(n_splits=n_folds, test_size=0.2, random_state=random_state)\n",
    "        scores = cross_val_score(model, x_train, y_train, cv=cv, fit_params={'callbacks': callbacks })\n",
    "        run_time = timer() - start\n",
    "\n",
    "        loss = max(scores)\n",
    "\n",
    "        # Round that returned the highest cv score\n",
    "        n_estimators = int(np.argmax(scores) + 1)\n",
    "\n",
    "        if ITERATION % 10 == 0:\n",
    "            # Display the information\n",
    "            display('Iteration {}: {} Fold CV Loss {:.5f}'.format(ITERATION, N_FOLDS, loss))\n",
    "\n",
    "        of_connection = open(hp_out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
    "        of_connection.close()\n",
    "\n",
    "        # Dictionary with information for evaluation\n",
    "        return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "                'estimators': n_estimators, \n",
    "                'train_time': run_time, 'status': STATUS_OK}\n",
    "\n",
    "    return _objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'dense_units': hp.choice('dense_units', [128, 256, 512, 1024, 2028]),\n",
    "    'lr': hp.loguniform('lr', np.log(0.00001), np.log(0.1)),\n",
    "    'dropout_rate_1': hp.choice('dropout_rate_1', [0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "    'l2_reg': hp.uniform('l1_reg', 0.0, 1.0),\n",
    "    'batch_size': hp.quniform('batch_size', 10, 100, 20),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 100\n",
    "\n",
    "tpe_algorithm = tpe.suggest\n",
    "bayes_trials = Trials()\n",
    "\n",
    "best = fmin(fn = objective(X_train_norm, y_train_norm), space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, rstate=np.random.RandomState(1138))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_trials.best_trial['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_nn((X.shape[1],), 1, dense_units=2028, l2_reg=0.964981502445198, dropout_rate_1=0.2, lr=0.00001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "model.reset_states()\n",
    "model.fit(X_train_norm, y_train_norm, epochs=100, batch_size=100, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(1,4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(X_train_norm.shape[1],), name='input_1')\n",
    "output_layer = Dense(1)(input_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='mse', optimizer='adagrad')\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train_norm[0:1], y_train_norm[0], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_train_norm[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_norm = y_scaler.fit_transform(y.reshape(-1, 1))\n",
    "y_scaler.lambdas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(y_norm), np.max(y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-3, 3, 11) \n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binned = np.digitize(y_norm, bins)\n",
    "y_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bincount = np.bincount(y_binned.ravel())\n",
    "bincount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bincount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 5, shuffle = True) \n",
    "for train_idx, test_idx in skf.split(X, y_binned):\n",
    "#     X_train = X.iloc[train_idx,:]\n",
    "#     y_train = y[train_idx]\n",
    "    print(train_idx.shape, test_idx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
