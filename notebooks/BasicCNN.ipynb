{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/will/Development/Astronomy/proposal/ENV/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "#from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import sdss_gz_data as sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalisation=True,\n",
    "                 conv_first=True\n",
    "                ):\n",
    "\n",
    "    def apply_normalisation(x):\n",
    "        if batch_normalisation:\n",
    "            x = BatchNormalization()(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def apply_activation(x):\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "#                   kernel_regularizer=l2(1e-4),\n",
    "                 )\n",
    "    \n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = apply_activation(apply_normalisation(conv(x)))\n",
    "    else:\n",
    "        x = conv(apply_activation(apply_normalisation(x)))\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_config(stage, res_block, num_filters_in):\n",
    "    num_filters_out = num_filters_in * 2\n",
    "    activation = 'relu'\n",
    "    batch_normalisation = True\n",
    "    strides = 1\n",
    "    \n",
    "    if stage == 0:\n",
    "        num_filters_out = num_filters_in * 4\n",
    "        if res_block == 0:\n",
    "            activation = None\n",
    "            batch_normalisation = False\n",
    "    else:\n",
    "        if res_block == 0:\n",
    "            strides = 2\n",
    "            \n",
    "    return num_filters_out, activation, batch_normalisation, strides\n",
    "\n",
    "def main_block(inputs, num_filters_in, num_filters_out, strides, activation, batch_normalisation):\n",
    "    y = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     kernel_size=1,\n",
    "                     strides=strides,\n",
    "                     activation=activation,\n",
    "                     batch_normalisation=batch_normalisation,\n",
    "                     conv_first=False)\n",
    "    y = resnet_layer(inputs=y,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=False)\n",
    "    y = resnet_layer(inputs=y,\n",
    "                     num_filters=num_filters_out,\n",
    "                     kernel_size=1,\n",
    "                     conv_first=False)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def residual_block(inputs, num_filters, strides):\n",
    "    return resnet_layer(inputs=inputs,\n",
    "                        num_filters=num_filters,\n",
    "                        kernel_size=1,\n",
    "                        strides=strides,\n",
    "                        activation=None,\n",
    "                        batch_normalisation=None\n",
    "                       )\n",
    "\n",
    "def resnetV2(inputs, depth, data_format='channels_last'):\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('Invalid depth, must be 9n+2')\n",
    "    \n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2)/9)\n",
    "    \n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters,\n",
    "                     conv_first=True,\n",
    "                     \n",
    "                    )\n",
    "    \n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            num_filters_out, activation, batch_normalisation, strides = get_config(stage, res_block, num_filters)\n",
    "            \n",
    "            y = main_block(inputs=x,\n",
    "                           num_filters_in=num_filters,\n",
    "                           num_filters_out=num_filters_out,\n",
    "                           strides=strides,\n",
    "                           activation=activation,\n",
    "                           batch_normalisation=batch_normalisation\n",
    "                          )\n",
    "            \n",
    "            if res_block == 0:\n",
    "                x = residual_block(x, num_filters_out, strides)\n",
    "                \n",
    "            x = keras.layers.add([x, y])\n",
    "        \n",
    "        num_filters = num_filters_out\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return GlobalAveragePooling2D(data_format=data_format)(x)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 42\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "num_classes = 2\n",
    "\n",
    "n_stages = 2\n",
    "depth = 9 * n_stages + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['objid', 'run', 'rerun', 'field', 'camcol', 'deVAB_i', 'expAB_g', 'expAB_i', 'expAB_z', 'expRad_g', 'expRad_u', 'expRad_z', 'fiberMag_g', 'fiberMag_u', 'fiberMag_z', 'model_g_u_colour_index', 'model_i_r_colour_index', 'model_r_g_colour_index', 'model_z_i_colour_index', 'petroRad_r', 'petro_R90_R50_ratio_g', 'petro_R90_R50_ratio_i', 'petro_r_g_colour_index', 'psfMag_r']\n"
     ]
    }
   ],
   "source": [
    "from sdss_gz_data import SPIRIAL_GALAXY_TYPE\n",
    "from sdss_gz_data import ELLIPTICAL_GALAXY_TYPE\n",
    "from sdss_gz_data import UNKNOWN_GALAXY_TYPE\n",
    "from sdss_gz_data import CONFIDENCE_LEVEL\n",
    "\n",
    "object_cols = [\n",
    "    'objid',\n",
    "    'run',\n",
    "    'rerun',\n",
    "    'field',\n",
    "    'camcol'\n",
    "]\n",
    "\n",
    "features = [\n",
    "    'deVAB_i',\n",
    "    'expAB_g',\n",
    "    'expAB_i',\n",
    "    'expAB_z',\n",
    "    'expRad_g',\n",
    "    'expRad_u',\n",
    "    'expRad_z',\n",
    "    'fiberMag_g',\n",
    "    'fiberMag_u',\n",
    "    'fiberMag_z',\n",
    "    'model_g_u_colour_index',\n",
    "    'model_i_r_colour_index',\n",
    "    'model_r_g_colour_index',\n",
    "    'model_z_i_colour_index',\n",
    "    'petroRad_r',\n",
    "    'petro_R90_R50_ratio_g',\n",
    "    'petro_R90_R50_ratio_i',\n",
    "    'petro_r_g_colour_index',\n",
    "    'psfMag_r'    \n",
    "]\n",
    "\n",
    "all_cols = object_cols + features\n",
    "print(all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = sgd.load_data('data/astromonical_data.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high z galaxies = 231\n",
      "Filtered out 3732 invalid records\n",
      "% elliptical:      0.13576908942272356\n",
      "% spiral:          0.2237143092857732\n",
      "% unknown:         0.6405166012915032\n",
      "% spiral of known: 0.6223216707350149\n"
     ]
    }
   ],
   "source": [
    "prepared_data = sgd.prepare_data(orig_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sgd.generate_features(use_averages=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = sgd.transform_data(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_data[all_cols]\n",
    "y = transformed_data[['galaxy_type','z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533332"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_galaxy_type_idx = transformed_data.galaxy_type != UNKNOWN_GALAXY_TYPE\n",
    "\n",
    "X = X[known_galaxy_type_idx]\n",
    "y = y[known_galaxy_type_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191724"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119314, 72410)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[y.galaxy_type == 0]), len(y[y.galaxy_type == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sgd.split_train(X, y, test_size=0.2, random_state=42)\n",
    "x_scaler = StandardScaler()\n",
    "x_scaler.fit(X_train[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = np.any([\n",
    "    X_train.index == 36370,\n",
    "    X_train.index == 25996,\n",
    "    X_train.index == 9620,\n",
    "    X_train.index == 519588,\n",
    "    X_train.index == 481146,\n",
    "    X_train.index == 60628,\n",
    "    X_train.index == 480839,\n",
    "    X_train.index == 480087\n",
    "], axis=0)\n",
    "\n",
    "X_train_small = X_train[selector]\n",
    "y_train_small = y_train[selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdss_gz_data import redshift_err\n",
    "from sdss_gz_data import z_err\n",
    "from sdss_gz_data import z_err_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "gcs_client = storage.Client()\n",
    "bucket = gcs_client.get_bucket('wgauvin-astroml-ast80014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download fits image\n",
    "def download_img(record, bucket):\n",
    "    run = record.run\n",
    "    camcol = record.camcol\n",
    "    field = record.field\n",
    "    objid = record.objid\n",
    "    \n",
    "    blob_dir = f'fits/{run}/{camcol}/{field}'\n",
    "    filename = f'obj-{objid}.fits.bz2'\n",
    "    blob_path = f'{blob_dir}/{filename}'\n",
    "    \n",
    "    print(f'Downloading {blob_path}')\n",
    "    \n",
    "    blob = bucket.get_blob(blob_path)\n",
    "    blob.download_to_filename(filename)\n",
    "\n",
    "    fits_file = fits.open(filename)\n",
    "    \n",
    "    return fits_file[0].data\n",
    "\n",
    "def augment_image(data):\n",
    "    return data\n",
    "\n",
    "def crop_image(data, image_size):\n",
    "    top_left = (72 - image_size)/2\n",
    "    bottom_right = top_left + image_size\n",
    "    \n",
    "    output_data = np.zeros((3, image_size, image_size))\n",
    "    \n",
    "    for idx in range(3):\n",
    "        img = Image.fromarray(data[idx])\n",
    "        img = img.crop((top_left, top_left, bottom_right, bottom_right))\n",
    "        output_data[idx] = np.array(img)\n",
    "    \n",
    "    return np.moveaxis(output_data, 0, -1)\n",
    "\n",
    "def get_image(record, bucket):\n",
    "    data = download_img(record, bucket)\n",
    "    data = augment_image(data)\n",
    "    return crop_image(data, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord, ICRS\n",
    "import astropy.units as u\n",
    "from astropy.nddata import Cutout2D\n",
    "from astropy.wcs import WCS\n",
    "from astropy.units import Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_images(images):\n",
    "    min_ = np.min(images)\n",
    "    max_ = np.max(images)\n",
    "\n",
    "    images = (images - min_)/(max_ - min_)\n",
    "    print(np.min(images), np.max(images))\n",
    "    \n",
    "    return images\n",
    "\n",
    "def generate_training_data(X_train, y_train, use_cnn=True, use_features=True):\n",
    "    def split(X, y):\n",
    "        X_t = {\n",
    "            'input_1': X['input_1'][0:6],\n",
    "            'input_2': X['input_2'][0:6]\n",
    "        }\n",
    "        X_v = {\n",
    "            'input_1': X['input_1'][6:8],\n",
    "            'input_2': X['input_2'][6:8]\n",
    "        }\n",
    "        y_t = {\n",
    "            'output_1': y['output_1'][0:6],\n",
    "            'output_2': y['output_2'][0:6],\n",
    "        }\n",
    "        y_v = {\n",
    "            'output_1': y['output_1'][6:8],\n",
    "            'output_2': y['output_2'][6:8],\n",
    "        }\n",
    "        \n",
    "        return X_t, y_t, X_v, y_v\n",
    "        \n",
    "    X = {  }\n",
    "    y = { 'output_1': y_train['galaxy_type'], 'output_2': y_train['z'] }\n",
    "    \n",
    "    if use_cnn:\n",
    "        images = np.ndarray((len(X_train), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=float)\n",
    "    \n",
    "        for idx, record in enumerate(X_train.itertuples()):\n",
    "            images[idx] = get_image(record, bucket)\n",
    "\n",
    "        X['input_1'] = normalise_images(images)\n",
    "        \n",
    "    if use_features:\n",
    "        X['input_2'] = x_scaler.transform(X_train[features])\n",
    "\n",
    "    return split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(input_1_shape, input_2_shape, dense_units=512, lr=0.00003, dropout=True, use_cnn=True, use_features=True):\n",
    "    if use_cnn:\n",
    "        input_1 = Input(shape=input_1_shape)\n",
    "        cnn = resnetV2(input_1, depth)\n",
    "\n",
    "    if use_features:\n",
    "        input_2 = Input(shape=(input_2_shape,), name='input_2')\n",
    "\n",
    "    if use_cnn:\n",
    "        if use_features:\n",
    "            x = keras.layers.concatenate([cnn, input_2])\n",
    "        else:\n",
    "            x = cnn\n",
    "    else:\n",
    "        if use_features:\n",
    "            x = input_2\n",
    "        else:\n",
    "            raise Exception('Need at least one input')\n",
    "    \n",
    "    # Make sure we normalise after concatination!!!\n",
    "    if (use_features and use_cnn):\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(dense_units,\n",
    "              kernel_initializer='random_normal',\n",
    "              name='hidden_layer_1',\n",
    "              use_bias=True,\n",
    "              activation='relu'\n",
    "             )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    if (dropout):\n",
    "        x = Dropout(0.1)(x)\n",
    "\n",
    "    x = Dense(dense_units,\n",
    "              kernel_initializer='random_normal',\n",
    "              name='hidden_layer_2',\n",
    "              use_bias=True,\n",
    "              activation='relu'\n",
    "             )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if (dropout):\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "    output_1 = Dense(1, # classification\n",
    "                     kernel_initializer='random_normal',\n",
    "                     name='output_1',\n",
    "                     use_bias=True,\n",
    "                     activation='sigmoid', \n",
    "                    )(x)\n",
    "    output_2 = Dense(1,\n",
    "                     kernel_initializer='random_normal',\n",
    "                     name='output_2',\n",
    "                     use_bias=True,\n",
    "                     activation='linear'\n",
    "                    )(x)\n",
    "\n",
    "    optimizer = Adam(lr=lr)\n",
    "    \n",
    "    outputs = [\n",
    "        output_1,\n",
    "        output_2\n",
    "    ]\n",
    "\n",
    "    if use_cnn:\n",
    "        if use_features:\n",
    "            inputs = [\n",
    "                input_1,\n",
    "                input_2\n",
    "            ]\n",
    "        else:\n",
    "            inputs = [ input_1 ]\n",
    "    else:\n",
    "        inputs = [ input_2 ]\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    loss_weights = { \n",
    "        'output_1': 1.0,\n",
    "        'output_2': 5.0\n",
    "    }\n",
    "    loss = {\n",
    "        'output_1': 'binary_crossentropy',\n",
    "        'output_2': redshift_err\n",
    "    }\n",
    "    metrics = {\n",
    "        'output_1': 'accuracy',\n",
    "        'output_2': redshift_err\n",
    "    }\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  loss_weights=loss_weights,\n",
    "                  metrics=metrics\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "#     output_1 = Dense(num_classes, name='output_1', activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x)\n",
    "#     output_2 = Dense(1, name='output_2', activation='linear', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x)\n",
    "#     return Model(inputs=[input_1, input_2], outputs=[output_1, output_2])\n",
    "    \n",
    "# K.clear_session()\n",
    "# model = create_nn(input_shape, len(features), dense_units=256, lr=0.0001, dropout=False, use_cnn=False)\n",
    "# model.summary()\n",
    "# init_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()\n",
    "#model.set_weights(init_weights)\n",
    "#model = create_nn(input_shape, len(features), dense_units=256, lr=0.001, dropout=False, use_cnn=False)\n",
    "# model.set_weights(init_weights)\n",
    "# model.summary()\n",
    "# model.fit(X_train_, y_train_, epochs=1000, batch_size=1)\n",
    "# X_train_, y_train_ = generate_training_data(X_train_small[0:1], y_train_small[0:1], use_cnn=False, use_features=False)\n",
    "#model.fit(X_train['input_2'], y_train['output_2'], epochs=10000, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, dropout=True, use_cnn=True, use_features=True, epochs=1000, batch_size=1, lr=0.1):\n",
    "    K.clear_session()\n",
    "    model = create_nn(input_shape,\n",
    "                      len(features),\n",
    "                      dense_units=256,\n",
    "                      lr=lr,\n",
    "                      dropout=dropout,\n",
    "                      use_cnn=use_cnn,\n",
    "                      use_features=use_features\n",
    "                     )\n",
    "    model.summary()\n",
    "    init_weights = model.get_weights()\n",
    "    \n",
    "    X_t, y_t, X_v, y_v = generate_training_data(X,\n",
    "                                                y,\n",
    "                                                use_cnn=use_cnn,\n",
    "                                                use_features=use_features\n",
    "                                               )\n",
    "    model.fit(X_t,\n",
    "              y_t,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(X_v, y_v)\n",
    "             )\n",
    "    \n",
    "    return model, init_weights, X_t, y_t, X_v, y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, init_weights, X_train_, y_train_, X_val_, y_val_ = train(\n",
    "    X_train_small,\n",
    "    y_train_small,\n",
    "    use_cnn=True,\n",
    "    use_features=True,\n",
    "    epochs=100,\n",
    "    lr=0.00003,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(X_t)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(result[0]).ravel().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t['output_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t['output_2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_['input_1'].shape, X_train_['input_2'].shape, y_train_['output_1'].shape, y_train_['output_2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = {\n",
    "    'input_1': X_train_['input_1'][0:6],\n",
    "    'input_2': X_train_['input_2'][0:6]\n",
    "}\n",
    "X_v = {\n",
    "    'input_1': X_train_['input_1'][6:8],\n",
    "    'input_2': X_train_['input_2'][6:8]\n",
    "}\n",
    "y_t = {\n",
    "    'output_1': y_train_['output_1'][0:6],\n",
    "    'output_2': y_train_['output_2'][0:6],\n",
    "}\n",
    "y_v = {\n",
    "    'output_1': y_train_['output_1'][6:8],\n",
    "    'output_2': y_train_['output_2'][6:8],\n",
    "}\n",
    "\n",
    "X_t['input_1'].shape, X_t['input_2'].shape, y_t['output_1'].shape, y_t['output_2'].shape, X_v['input_1'].shape, X_v['input_2'].shape, y_v['output_1'].shape, y_v['output_2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train_['input_1']), np.max(X_train_['input_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result[1][0] - y_train_['output_2'])/(1 + y_train_['output_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_err_val = np.average(np.abs((y_train_['output_2'] - result[1][0])/(1 + y_train_['output_2'])))\n",
    "z_err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_['output_2'].values, result[1])\n",
    "z_err_stats(y_train_['output_2'].values, result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_err(result[1], y_train_['output_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_indexes = X_train.index.values\n",
    "\n",
    "indexes = np.arange(len(list_of_indexes))\n",
    "\n",
    "np.random.shuffle(indexes)\n",
    "print(list_of_indexes, indexes)\n",
    "list_of_indexes[indexes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "galaxy_image_generator = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=360,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# need to crop data\n",
    "def crop_image(image, image_size):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train[X_train.index == 480839].to_records()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train.loc[480839]))\n",
    "print(X_train.loc[480839]['objid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train[X_train.index == 480839]\n",
    "X_train_small.loc[480839]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.loc[480839]['galaxy_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train_small.loc[480839]))\n",
    "X_train_small.loc[480839].objid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, record in enumerate(X_train_small.itertuples()):\n",
    "    print(record.objid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 6\n",
    "\n",
    "idx_range = range(6)\n",
    "print(idx_range)\n",
    "\n",
    "\n",
    "X_train_small.iloc[idx_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del range\n",
    "for idx in range(6):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
